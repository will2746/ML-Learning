{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIiq2o/aAZPRKD+KO7tQWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/will2746/ML-Learning/blob/main/Big_Daddy_Ai_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4xEFYaPkG-v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An AI Research Paper Summarizer"
      ],
      "metadata": {
        "id": "h863mBj_kJv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade openai\n",
        "! pip install PyMuPDF\n",
        "! pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KePvCn6NkQnV",
        "outputId": "c82fbc5c-d383-4c26-991a-7b2fc700f72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.25.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.8/dist-packages (from openai) (1.5.2.221213)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.7)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.8/dist-packages (from pandas-stubs>=1.1.0.11->openai) (2022.7.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.8/dist-packages (1.21.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. importing stuff"
      ],
      "metadata": {
        "id": "nWnNdMSIkSqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Importing a document:\n",
        "\n",
        "# Import the fitz module from PyMuPDF\n",
        "import os\n",
        "import fitz \n",
        "import math\n",
        "import openai\n",
        "import re\n",
        "import os\n",
        "import transformers\n",
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "M7rmgcmEkhA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the file location"
      ],
      "metadata": {
        "id": "20p5LAlrkast"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the Pdf Document folder\n",
        "pdf_path_in ='/content/cleantechnol-04-00029.pdf'"
      ],
      "metadata": {
        "id": "jXOGE2enk3Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All the Functions"
      ],
      "metadata": {
        "id": "uinIvCgxk72J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Creating code that will extract the PDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Iterate through each page of the PDF\n",
        "    text1=''\n",
        "    for page in pdf_document:\n",
        "        # Extract the text from the page\n",
        "        text = page.get_text(\"text\")\n",
        "        text1= text1 + text\n",
        "\n",
        "    return text1\n",
        "\n",
        "#this gets rid of any refrences at the end\n",
        "def delete_after_references(text):\n",
        "    # Split the text into a list of lines\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    # Find the index of the line containing \"References\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"References\" in line:\n",
        "            index = i\n",
        "            break\n",
        "\n",
        "    # Delete all lines after the line containing \"References\"\n",
        "    lines = lines[:index+1]\n",
        "\n",
        "    # Join the lines back into a single string\n",
        "    text = \"\\n\".join(lines)\n",
        "    return text\n",
        "\n",
        "# Finding headings in a text file:\n",
        "\n",
        "def find_headings(document_text):\n",
        "    # Open the text file and read its contents into a string\n",
        "    lines = []\n",
        "    headers = '1'\n",
        "    beans = 0\n",
        "\n",
        "    while headers:\n",
        "        beans = beans+1\n",
        "    # Use the regular expression to find all lines that start with \"1.\"\n",
        "        headers = re.findall(r'^{}\\.\\s+(.*)'.format(beans), document_text, re.M)\n",
        "\n",
        "        headers = ' '.join(headers)\n",
        "        # need to account for roman numerals as well here... for another day...\n",
        "\n",
        "        header1= str(beans) + \". \" + headers\n",
        "        lines.append(header1)\n",
        "\n",
        "    # Print the lines\n",
        "    lines.pop()\n",
        "    lines.insert(0, 'Abstract')\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "# this function takes a text file and a list of headers, it looks through the\n",
        "# paper for the headings, then it takes the headers and it finds the text in those sections\n",
        "# It stores them in a list for later\n",
        "def get_text_between(file_contents,paper_headings):\n",
        "    # Check if the file exists\n",
        "    output={}\n",
        "    for i in range(len(paper_headings)):\n",
        "\n",
        "        if i < len(paper_headings)-1:\n",
        "            # Find the starting index of the text\n",
        "            start_index = file_contents.find(paper_headings[i])\n",
        "\n",
        "            # Find the ending index of the text\n",
        "            end_index = file_contents.find(paper_headings[i+1])\n",
        "        else:\n",
        "            # Find the starting index of the text\n",
        "            start_index = file_contents.find(paper_headings[i])\n",
        "\n",
        "            # Find the ending index of the text\n",
        "            end_index = len(file_contents)\n",
        "\n",
        "        # Extract the text between the starting and ending indices\n",
        "        text = file_contents[start_index:end_index]\n",
        "        output[paper_headings[i]] = text\n",
        "\n",
        "    return output\n",
        "\n",
        "def split_dictionary_values(d):\n",
        "  # Import the BertTokenizer class\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Initialize the BertTokenizer\n",
        "  tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "  # Create a new dictionary to store the updated key-value pairs\n",
        "  d_new = {}\n",
        "\n",
        "  # Iterate over the key-value pairs in the dictionary\n",
        "  for key, value in d.items():\n",
        "    # Tokenize the value\n",
        "    tokens = tokenizer.tokenize(value)\n",
        "\n",
        "    # If the value is over 4000 tokens, split it in half and add the two halves to the new dictionary\n",
        "    if len(tokens) > 3000:\n",
        "      half = len(tokens) // 2\n",
        "      # Encode the value as a string to get the token IDs\n",
        "      token_ids = tokenizer.encode(value)\n",
        "      # Split the token IDs in half and decode them\n",
        "      value1 = tokenizer.decode(token_ids[:half])\n",
        "      value2 = tokenizer.decode(token_ids[half:])\n",
        "      # Add the two halves to the new dictionary\n",
        "      d_new[key + \" part 1\"] = value1\n",
        "      d_new[key + \" part 2\"] = value2\n",
        "    else:\n",
        "      # If the value is not over 4000 tokens, just add it to the new dictionary\n",
        "      d_new[key] = value\n",
        "\n",
        "  # Replace the original dictionary with the new one\n",
        "  d = d_new\n",
        "  return d\n",
        "      # Recursively call the function on the modified dictionary\n",
        "\n",
        "# Test the function\n",
        "\n",
        "# Running GPT-3\n",
        "def create_summaries(input_text_path):\n",
        "\n",
        "    # Set the API key\n",
        "    final_text=''\n",
        "    openai.api_key = \"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\"\n",
        "    for key, value in input_text_path.items():\n",
        "\n",
        "        # Set the prompt\n",
        "        prompt = (\n",
        "            f\"Section Title: {key}\\n\\n\"\n",
        "            f\"{value}\\n\\n\"\n",
        "            \"please summarize this excerpt from a scientific research paper in as much detail as possible, if applicable, include any key findings and suggestions for future research\\n\"\n",
        "        )\n",
        "\n",
        "        # Set the number of completions\n",
        "        num_completions = 1\n",
        "\n",
        "        # Generate completions\n",
        "        completions = openai.Completion.create(\n",
        "            engine='text-davinci-003',\n",
        "            prompt=prompt,\n",
        "            max_tokens=1500,\n",
        "            temperature=.7,\n",
        "            top_p=.8)\n",
        "        final_text=final_text+completions.choices[0].text\n",
        "\n",
        "        # Return the first completion\n",
        "    return final_text\n",
        "\n",
        "\n",
        "    #Summarize the whole paper:\n",
        "def summarize_paper(input_text):\n",
        "\n",
        "    # Set the API key\n",
        "    openai.api_key = \"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\"\n",
        "\n",
        "    # Set the prompt\n",
        "    prompt = ('provide a summary of these summaries from a paper include key findings and suggestions for future reserach'+input_text\n",
        "\n",
        "    )\n",
        "\n",
        "    # Set the number of completions\n",
        "    num_completions = 1\n",
        "\n",
        "    # Generate completions\n",
        "    completions = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        max_tokens=1000,\n",
        "        temperature=.7,\n",
        "        top_p=.8)\n",
        "\n",
        "      # Return the first completion\n",
        "    print(completions.choices[0].text)\n",
        "    return completions.choices[0].text\n",
        "\n",
        "\n",
        "    # Print the completion\n",
        "\n",
        "def big_daddy(pdf_file_location):\n",
        "\n",
        "  # Running the first function to extract text from the PDF\n",
        "  text_file=extract_text_from_pdf(pdf_path_in)\n",
        "\n",
        "  #this file gets rid of all the text after the refrences (v important!)\n",
        "  document_text=delete_after_references(text_file)\n",
        "\n",
        "  # finding the headings of the paper (kinda genious)\n",
        "  paper_headings = find_headings(document_text)\n",
        "\n",
        "  # This function breaks the text into sections by Abstract, intro and so on\n",
        "  broked_text= get_text_between(document_text, paper_headings)\n",
        "\n",
        "  machine= split_dictionary_values(broked_text)\n",
        "\n",
        "  # This calles OpenAi to summarize the text\n",
        "  paper_summaries=create_summaries(machine)\n",
        "\n",
        "  #this one summarizes the summaries\n",
        "  summarize_paper(paper_summaries)\n",
        "\n",
        "big_daddy(pdf_path_in)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPjW1jblkYt_",
        "outputId": "a5d72b38-63f9-4d38-b60b-885f02175c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1604 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Big Daddy Function that runs everything!"
      ],
      "metadata": {
        "id": "BrXJ4Ch4lHxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "big_daddy(pdf_path_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "H1_RsebElHS9",
        "outputId": "df791b0d-263b-4cc8-f25a-c8680a75c767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Abstract', '1. Introduction', '2. Energy Storage Resources', '3. Energy Storage Technologies', '4. Energy Storage Modeling', '5. Discussions', '6. Conclusions', '7. Challenges and Future Works']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1068\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[0;32m-> 1376\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-63b65431-56d6ea85626cf7c167d300ed)\n\nRepository Not Found for url: https://huggingface.co/gpt3/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-44d177dafa85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbig_daddy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-d8c1feb47ab2>\u001b[0m in \u001b[0;36mbig_daddy\u001b[0;34m(pdf_file_location)\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0mbroked_text\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_text_between\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper_headings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m   \u001b[0mmachine\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroked_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-d8c1feb47ab2>\u001b[0m in \u001b[0;36msplit_text\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;31m# Initialize the GPT2TokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;31m# Initialize a list to store the split text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                     \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: gpt3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
          ]
        }
      ]
    }
  ]
}